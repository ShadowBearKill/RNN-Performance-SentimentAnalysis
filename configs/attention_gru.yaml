# Self-Attention GRU模型实验配置
model:
  name: "AttentionGRUModel"
  version: "v1"
  embed_dim: 128
  hidden_dim: 256
  n_heads: 4            # 新增：多头注意力的头数
  output_dim: 2
  n_layers: 1
  bidirectional: true
  dropout: 0.5

training:
  learning_rate: 0.001
  batch_size: 32
  epochs: 40
  device: "cuda"
  patience: 4

data:
  path: "data/ChnSentiCorp_htl_all.csv"
  min_freq: 10 