# Self-Attention GRU模型实验配置
model:
  name: "AttentionGRUModel"
  embed_dim: 128
  hidden_dim: 256
  output_dim: 2
  n_layers: 1
  bidirectional: true
  dropout: 0.5

training:
  learning_rate: 0.001
  batch_size: 64
  epochs: 15
  device: "cuda"
  patience: 3

data:
  path: "data/ChnSentiCorp_htl_all.csv"
  min_freq: 10 